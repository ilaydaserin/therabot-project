from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Model ve Tokenizer yÃ¼kleme
model_name = "gpt2-fine_tuned_newintents/gpt2-fine_tuned_newintents_model"  # EÄŸitilmiÅŸ model dosyanÄ±zÄ±n yolu
tokenizer = GPT2Tokenizer.from_pretrained(model_name)
model = GPT2LMHeadModel.from_pretrained(model_name)

# Tokenizer iÃ§in padding token ayarÄ±
tokenizer.pad_token = tokenizer.eos_token

# Chatbot fonksiyonu
def chatbot(user_input):
    # Girdi metnini formatla
    input_text = f"User: {user_input}\nBot:"
    input_ids = tokenizer.encode(input_text, return_tensors="pt").to(model.device)  # GPU'ya taÅŸÄ±

    # Modelle yanÄ±t oluÅŸtur
    output = model.generate(
        input_ids=input_ids,
        max_length=100,              # Maksimum yanÄ±t uzunluÄŸu
        num_return_sequences=1,      # Tek yanÄ±t dÃ¶ndÃ¼r
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,              # Ã–rnekleme kullan
        temperature=0.7,             # YaratÄ±cÄ±lÄ±ÄŸÄ± kontrol eder
        top_k=40,                    # Ä°lk 40 olasÄ± kelime arasÄ±ndan seÃ§im
        top_p=0.9,                   # Nucleus sampling
        repetition_penalty=1.2       # Tekrar eden ifadeleri azalt
    )

    # YanÄ±tÄ± decode ederek temizle
    response = tokenizer.decode(output[0], skip_special_tokens=True)
    response = response.split("Bot:")[-1].strip()  # "Bot:" kÄ±smÄ±ndan sonrasÄ±
    response = response.split("\n")[0]  # Sadece ilk satÄ±rÄ± al
    return response


# Sohbet testi
if __name__ == "__main__":
    print("Therabot: Merhaba! YardÄ±m etmek iÃ§in buradayÄ±m. Ã‡Ä±kmak iÃ§in 'exit' yazabilirsiniz.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("Therabot: GÃ¶rÃ¼ÅŸmek Ã¼zere! Kendinize iyi bakÄ±n. ğŸ˜Š")
            break
        try:
            response = chatbot(user_input)
            print(f"Bot: {response}")
        except Exception as e:
            print(f"Therabot: Bir hata oluÅŸtu. {str(e)}")
